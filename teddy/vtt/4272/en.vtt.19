WEBVTT

00:09:26.978 --> 00:09:31.361
reflection in somebody's glasses of the smartphone

00:09:31.361 --> 00:09:32.786
that they're typing in.

00:09:32.786 --> 00:09:34.761
They wrote software to stabilize --

00:09:34.761 --> 00:09:36.126
even though they were on a bus

00:09:36.126 --> 00:09:39.337
and maybe someone's holding their phone at an angle --

00:09:39.337 --> 00:09:41.707
to stabilize the phone, process it, and

00:09:41.707 --> 00:09:43.592
you may know on your smartphone, when you type

00:09:43.592 --> 00:09:46.531
a password, the keys pop out a little bit, and they were able

00:09:46.531 --> 00:09:49.371
to use that to reconstruct what the person was typing,

00:09:49.371 --> 00:09:53.692
and had a language model for detecting typing.

00:09:53.692 --> 00:09:56.027
What was interesting is, by videotaping on a bus,

00:09:56.027 --> 00:09:58.156
they were able to produce exactly what people

00:09:58.156 --> 00:10:00.307
on their smartphones were typing,