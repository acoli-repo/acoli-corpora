WEBVTT

00:05:01.715 --> 00:05:05.497
Jeremy Howard: Esto era una conferencia
de aprendizaje automático en China.

00:05:05.497 --> 00:05:07.867
No es usual, en conferencias académicas

00:05:07.867 --> 00:05:09.764
oír aplausos espontáneos,

00:05:09.764 --> 00:05:12.830
aunque en las conferencias
de TEDx siéntanse libres.

00:05:12.830 --> 00:05:15.115
Todo lo que han visto es gracias
al aprendizaje profundo.

00:05:15.115 --> 00:05:16.880
(Aplausos) Gracias.

00:05:16.880 --> 00:05:19.722
La transcripción en inglés
es aprendizaje profundo.

00:05:19.722 --> 00:05:23.204
La traducción al chino y el texto arriba
a la derecha, es aprendizaje profundo,

00:05:23.204 --> 00:05:26.041
y la construcción de la voz también
es aprendizaje profundo.

00:05:26.761 --> 00:05:29.995
Eso es lo extraordinario
del aprendizaje profundo.

00:05:29.995 --> 00:05:33.094
Es un solo algoritmo que parece
hacer casi cualquier cosa,