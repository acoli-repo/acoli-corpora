WEBVTT

00:57:59.351 --> 00:58:00.974
and then, the question is:

00:58:00.998 --> 00:58:05.180
What moral biases and intuitions
do you want to build into your robot cars?

00:58:05.204 --> 00:58:10.088
Do you want cars that run over
white people preferentially

00:58:10.112 --> 00:58:12.545
because of all the white
privilege in the world?

00:58:12.569 --> 00:58:19.465
Do you want cars that put
the passenger's life at some greater risk,

00:58:19.489 --> 00:58:24.124
if we're talking about a trolley problem,
where it's the one versus the five,

00:58:24.148 --> 00:58:25.363
or the one versus the 10.

00:58:25.387 --> 00:58:27.220
CA: One child versus three old people.

00:58:27.244 --> 00:58:30.713
SH: Exactly. And to not answer
these questions