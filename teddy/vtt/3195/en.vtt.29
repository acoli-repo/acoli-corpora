WEBVTT

00:14:28.320 --> 00:14:32.496
This is the gist of what Eliezer Yudkowsky
has called "friendly AI,"

00:14:32.520 --> 00:14:35.200
and if we can do this,
it could be awesome.

00:14:35.840 --> 00:14:40.656
It could not only eliminate negative
experiences like disease, poverty,

00:14:40.680 --> 00:14:42.136
crime and other suffering,

00:14:42.160 --> 00:14:44.976
but it could also give us
the freedom to choose

00:14:45.000 --> 00:14:49.056
from a fantastic new diversity
of positive experiences --

00:14:49.080 --> 00:14:52.240
basically making us
the masters of our own destiny.

00:14:54.280 --> 00:14:55.656
So in summary,

00:14:55.680 --> 00:14:58.776
our situation with technology
is complicated,

00:14:58.800 --> 00:15:01.216
but the big picture is rather simple.