WEBVTT

00:10:27.480 --> 00:10:31.096
we need to figure out how to transform
today's buggy and hackable computers

00:10:31.120 --> 00:10:33.536
into robust AI systems
that we can really trust,

00:10:33.560 --> 00:10:34.776
because otherwise,

00:10:34.800 --> 00:10:37.616
all this awesome new technology
can malfunction and harm us,

00:10:37.640 --> 00:10:39.616
or get hacked and be turned against us.

00:10:39.640 --> 00:10:45.336
And this AI safety work
has to include work on AI value alignment,

00:10:45.360 --> 00:10:48.176
because the real threat
from AGI isn't malice,

00:10:48.200 --> 00:10:49.856
like in silly Hollywood movies,

00:10:49.880 --> 00:10:51.616
but competence --

00:10:51.640 --> 00:10:55.056
AGI accomplishing goals
that just aren't aligned with ours.

00:10:55.080 --> 00:10:59.816
For example, when we humans drove
the West African black rhino extinct,

00:10:59.840 --> 00:11:03.736
we didn't do it because we were a bunch
of evil rhinoceros haters, did we?